{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "CSV_LOC = 'results/predictions_dlibResnet_threshold_0.4.csv'\n",
    "NAME = CSV_LOC[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_source_cols(df):\n",
    "    \"\"\"\n",
    "    Given a dataframe df, create all necessary source columns.\n",
    "    \n",
    "    The input dataframe should have a column named source_class from which each row\n",
    "    has a name in the form of\n",
    "        'class_sess_pose_illum_expr_pitch_yaw_roll.extension'\n",
    "    \n",
    "    The output is a copy of the original dataframe with added columns.\n",
    "\n",
    "    NOTE: the if/else is required because targets can have a name of '-1_'.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    for name in ['source_name', 'target_name']:\n",
    "        n = name.split('_')[0] + '_'\n",
    "\n",
    "        df[n + 'class'] = df[name].apply(lambda x: int(x.split('_')[0]) if (x != '-1_') else None)\n",
    "        df[n + 'session'] = df[name].apply(lambda x: float(x.split('_')[1]) if (x != '-1_') else None)\n",
    "\n",
    "        df[n + 'pose'] = df[name].apply(lambda x: float(x.split('_')[2]) if (x != '-1_') else None)\n",
    "        df[n + 'illumination'] = df[name].apply(lambda x: float(x.split('_')[3]) if (x != '-1_') else None)\n",
    "        df[n + 'expression'] = df[name].apply(lambda x: float(x.split('_')[4]) if (x != '-1_') else None)\n",
    "\n",
    "\n",
    "        df[n + 'pitch'] = df[name].apply(lambda x: float(x.split('_')[5][1:]) if (x != '-1_') else None)\n",
    "        df[n + 'yaw'] = df[name].apply(lambda x: float(x.split('_')[6][1:]) if (x != '-1_') else None)\n",
    "        df[n + 'roll'] = df[name].apply(lambda x: float(x.split('_')[7].split('.')[0][1:]) if (x != '-1_') else None)\n",
    "\n",
    "        # Illumination changed, modify this and uncomment the previous roll\n",
    "        #df[n + 'roll'] = df[name].apply(lambda x: float(x.split('_')[7][1:]) if (x != '-1_') else None)\n",
    "        #df[n + 'illum_augmented'] = df[name].apply(lambda x: float(x.split('_')[8][1:] if (x 1= '-1_') else None))\n",
    "        #df[n + 'intensity_augmented'] = df[name].apply(lambda x: float(x.split('_')[8].split('.')[0][1:] if (x 1= '-1_') else None))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results analysis\n",
    "1. create the dataframe for a CSV results file at ```CSV_LOC```.\n",
    "2. Append all possible variables (pose, illumination etc), extracted from both source and target name through ```create_source_cols```.\n",
    "2. Next, generate some interesting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_LOC)\n",
    "\n",
    "# TESTING\n",
    "df = df.append({'source_name': '005_66_051_17_0_p0_y0_r0.png',\n",
    " 'target_name': '005_66_051_17_0_p5.4_y4.3_r7.5.png',\n",
    " 'distance': .666, 'confidence': None, 'predicted_class': 5,\n",
    " 'true_class': 5, 'correct': True}, ignore_index=True)\n",
    "df = df.append({'source_name': '006_67_051_17_0_p0_y0_r0.png',\n",
    " 'target_name': '009_67_051_17_0_p5.4_y0.0_r0.0.png',\n",
    " 'distance': .666, 'confidence': None, 'predicted_class': 9,\n",
    " 'true_class': 6, 'correct': True}, ignore_index=True)\n",
    "# END TESTING\n",
    "\n",
    "\n",
    "# Note that the below edits should not be problematic, they should always be true\n",
    "\n",
    "## If predicted class is -1 for true targets, it should always be false\n",
    "zoom = df[df['true_class'] <= 100]['predicted_class'] == -1\n",
    "zoom = zoom.where(zoom == True).dropna()\n",
    "df.loc[zoom.keys(), 'correct'] = False\n",
    "\n",
    "## \n",
    "zoom = df[df['true_class'] > 100]['predicted_class'] == -1\n",
    "zoom = zoom.where(zoom == True).dropna()\n",
    "df.loc[zoom.keys(), 'correct'] = True\n",
    "\n",
    "new_df = create_source_cols(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy of the model: {new_df['correct'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For false positives etc we need only concern ourselves with wrong classifications\n",
    "false_df = new_df[new_df['correct'] == False]\n",
    "false_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of wrongly classified images\n",
    "\n",
    "Where does the model make mistakes? A simple histogram for false positives and false negatives can perhaps shed some light into this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos = false_df[(false_df['source_class'].astype(int) >  100)]\n",
    "false_neg = false_df[(false_df['source_class'].astype(int) <= 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "\n",
    "sns.distplot(false_pos['source_class'].astype(int),\n",
    "             norm_hist=False,\n",
    "             kde=False,\n",
    "             bins=false_pos['source_class'].nunique(),\n",
    "             label=f\"False Positives, num = {false_pos['source_class'].count()}\"\n",
    "            )\n",
    "sns.distplot(false_neg['source_class'].astype(int),\n",
    "             norm_hist=False,\n",
    "             kde=False,\n",
    "             bins=false_neg['source_class'].nunique(),\n",
    "             label=f\"False Negatives, num = {false_neg['source_class'].count()}\"\n",
    "            )\n",
    "\n",
    "plt.xlabel(\"Source Class\", fontsize=20) ; plt.ylabel('Number of Errors', fontsize=20)\n",
    "plt.legend(fontsize=15)\n",
    "#plt.savefig(NAME + \"_hist_fp_fn.png\", dpi=400)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_df) - sum(new_df['true_class'] > 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Negatives\n",
    "\n",
    "Let's look more into the false negatives, false positives should be able toe be removed mostly through the use of thresholding, but false negatives can be quite a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize = (20, 6))\n",
    "\n",
    "plt.subplot(131)  # EXPRESSION\n",
    "uniqs_expres = sorted(false_neg['source_expression'].astype(int).unique())\n",
    "expres_dict = {uniq.astype(int): sum(false_neg['source_expression'].astype(int) == uniq) for uniq in uniqs_expres}\n",
    "plt.bar(x = range(len(uniqs_expres)), height = expres_dict.values(), alpha=0.5)\n",
    "plt.xticks(range(len(uniqs_expres)), uniqs_expres)\n",
    "plt.xlabel(\"Expression type\", fontsize=20) ; plt.ylabel(\"Number of Errors\", fontsize=20);\n",
    "\n",
    "plt.subplot(132)  # POSE\n",
    "uniqs_pose = np.array([80, 130, 140, 51, 50, 41, 190])\n",
    "pose_dict = {uniq.astype(int): sum(false_neg['source_pose'].astype(int) == uniq) for uniq in uniqs_pose}\n",
    "plt.bar(x = range(len(uniqs_pose)), height = pose_dict.values(), alpha=0.5)\n",
    "plt.xticks(range(len(uniqs_pose)), uniqs_pose)\n",
    "plt.xlabel(\"Pose angle\", fontsize=20) ; plt.ylabel(\"Number of Errors\", fontsize=20);\n",
    "\n",
    "plt.subplot(133)  # ILLUMINATION\n",
    "uniqs_illum = np.array([2, 7, 17, 12])\n",
    "illumination_dict = {uniq.astype(int): sum(false_neg['source_illumination'].astype(int) == uniq) for uniq in uniqs_illum}\n",
    "plt.bar(x = range(len(uniqs_illum)), height = illumination_dict.values(), alpha=0.5)\n",
    "plt.xticks(range(len(uniqs_illum)), uniqs_illum)\n",
    "plt.xlabel(\"Illumination type\", fontsize=20) ; plt.ylabel(\"Number of Errors\", fontsize=20);\n",
    "\n",
    "#plt.savefig(NAME + \"_hist_exp_pose_ill.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the effect of augmented yaw/pitch/roll/illumination/intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_yprii(df):\n",
    "    \"\"\"In order to understand how augmented increases/decreases the accuracy,\n",
    "    filter out any target prediction on these. The default values are None/nan under pandas.\n",
    "    TODO: when new files arrive, uncommend the commented sectionand comment the currently uncommented\"\"\"\n",
    "    \n",
    "    df = df[df['correct'] == True]\n",
    "    \n",
    "    # First filter out any nans\n",
    "    filter_nans = ~ (pd.isna(df['target_pitch']) | pd.isna(df['target_yaw']) | pd.isna(df['target_roll']))\n",
    "    \n",
    "#     filter_nans = ~ (pd.isna(df['target_pitch']) |\n",
    "#                      pd.isna(df['target_yaw'])   |\n",
    "#                      pd.isna(df['target_roll'])  | \n",
    "#                      pd.isna(df['target_illumination_augmented']) |\n",
    "#                      pd.isna(df['target_intensity_augmented']))\n",
    "    \n",
    "    # Second, only select where at least one attribute has been changed\n",
    "    filter_0 = ~ (df['target_pitch'] + df['target_yaw'] + df['target_roll'] == 0.0)\n",
    "#     filter_0 = ~ (df['target_pitch']        + \n",
    "#                   df['target_yaw']          + \n",
    "#                   df['target_roll']         + \n",
    "#                   df['target_illumination_augmented'] +\n",
    "#                   df['target_intensity_augmented'] == 0.0)\n",
    "    \n",
    "    f = filter_nans & filter_0\n",
    "    print(f\"{len(df[f])/len(df[~f]):.6f}% of correctly classified matched to augmented data (n={len(df[f])}).\")\n",
    "    \n",
    "    return df[f]\n",
    "\n",
    "filtered_yprii = filter_yprii(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_yprii[['target_pitch', 'target_yaw', 'target_roll']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_yprii[['target_pitch', 'target_yaw', 'target_roll']].hist(\n",
    "    figsize=(20, 4),\n",
    "    sharey=True,\n",
    "    layout=(1, 3)) ;\n",
    "\n",
    "# filtered_yprii[['target_pitch', 'target_yaw', 'target_roll',\n",
    "#                 'target_illumination_augmented',\n",
    "#                 'target_intensity_augmented']].hist(\n",
    "#     figsize=(25, 4),\n",
    "#     sharey=True,\n",
    "#     layout=(1, 5)) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (PAI)",
   "language": "python",
   "name": "pai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
